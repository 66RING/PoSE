{
   "version": "0.2.0",
   "configurations": [
       {
           "type": "python",
           "request": "launch",
           "name": "Launch",
		   "program": "${file}",
		   "args": [
			  "--output_dir", "../model_and_data/pose/results/4k-128k-yarn",
			  "--model_name_or_path", "../model_and_data/model/Llama-2-7b-chat-hf",
			  "--train_data_path", "../model_and_data/data/gov-report-qs/train.jsonl",
			  "--valid_data_path", "../model_and_data/data/gov-report-qs/valid.jsonl",
			  "--test_data_path", "../model_and_data/data/gov-report-qs/test.jsonl",
			  "--max_steps", "1000",
			  "--model_max_position_embeddings", "2048",
			  "--rope_scaling_type", "yarn",
			  "--rope_scaling_factor", "128",
			  "--inference_length", "16384",
			  "--per_device_train_batch_size", "8",
			  "--per_device_eval_batch_size", "1",
			  "--gradient_accumulation_steps", "1",
			  "--do_train", "True",
			  "--do_eval", "True",
			  "--do_predict", "True",
			  "--evaluation_strategy", "steps",
			  "--eval_steps", "50",
			  "--save_strategy", "steps",
			  "--save_steps", "100",
			  "--load_best_model_at_end", "True",
			  "--learning_rate", "2e-5",
			  "--warmup_steps", "10",
			  "--logging_steps", "5",
			  "--report_to", "'tensorboard'",
			  "--gradient_checkpointing", "True",
			  "--fp16", "True"
		   ]
       }
   ]
}

